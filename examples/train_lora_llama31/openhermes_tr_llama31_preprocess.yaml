### model
model_name_or_path: meta-llama/Llama-3.1-8B-Instruct

### method
stage: sft
do_train: true
finetuning_type: lora
lora_target: all

### dataset
dataset: nbs_data
template: llama3
cutoff_len: 1024
max_samples: 1000
overwrite_cache: true
preprocessing_num_workers: 16
#tokenized_path: to load or save tokenized data
#saving tokenized data at the '/opt/ml/model' path so that
# Pytorch estimator will sync the data to s3 output path
tokenized_path: /opt/ml/model


### output
output_dir: /opt/ml/model
overwrite_output_dir: true